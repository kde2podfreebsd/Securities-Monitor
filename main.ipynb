{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import enum\n",
    "import json\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date, timedelta, datetime\n",
    "from typing import List, Dict, Union, Any, Optional, Date\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassportMOEXAuth:\n",
    "    _instance: 'PassportMOEXAuth' = None\n",
    "\n",
    "    def __new__(cls, *args, **kwargs) -> 'PassportMOEXAuth':\n",
    "        \"\"\"\n",
    "        Создает и возвращает единственный экземпляр класса (реализует паттерн Singleton).\n",
    "        \"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(PassportMOEXAuth, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, username: str, password: str) -> None:\n",
    "        \"\"\"\n",
    "        Инициализирует объект аутентификации с заданными именем пользователя и паролем.\n",
    "        \n",
    "        :param username: Имя пользователя для аутентификации\n",
    "        :param password: Пароль для аутентификации\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'initialized'):\n",
    "            self.auth_cert: str | None = None\n",
    "            self.error_count: int = 0\n",
    "            self.username: str = username\n",
    "            self.password: str = password\n",
    "            self.initialized: bool = True\n",
    "\n",
    "    def authenticate(self) -> bool:\n",
    "        \"\"\"\n",
    "        Аутентифицирует пользователя на сервере MOEX и сохраняет аутентификационный сертификат.\n",
    "\n",
    "        :return: Возвращает True, если аутентификация прошла успешно, иначе False.\n",
    "        \"\"\"\n",
    "        AUTH_URL: str = 'https://passport.moex.com/authenticate'\n",
    "        try:\n",
    "            response: httpx.Response = httpx.get(AUTH_URL, auth=(self.username, self.password), timeout=600)\n",
    "            if response.status_code == 200:\n",
    "                self.auth_cert = response.cookies.get('MicexPassportCert')\n",
    "                self.error_count = 0\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except httpx.RequestError as exc:\n",
    "            print(f\"An error occurred while requesting {exc.request.url!r}.\")\n",
    "            return False\n",
    "\n",
    "    def auth_request(self, url: str) -> dict:\n",
    "        \"\"\"\n",
    "        Выполняет авторизованный запрос к заданному URL и возвращает ответ в формате JSON.\n",
    "        \n",
    "        :param url: URL для выполнения запроса\n",
    "        :return: Ответ в формате JSON\n",
    "        :raises Exception: Если количество ошибок аутентификации превышает 3.\n",
    "        \"\"\"\n",
    "        if self.auth_cert is None:\n",
    "            self.authenticate()\n",
    "\n",
    "        headers: dict[str, str] = {'Cookie': f'MicexPassportCert={self.auth_cert}', 'Cache-Control': 'no-cache'}\n",
    "        \n",
    "        try:\n",
    "            response: httpx.Response = httpx.get(url, headers=headers, timeout=600) \n",
    "            if response.status_code == 200:\n",
    "                self.error_count = 0\n",
    "                return response.json()\n",
    "            else:\n",
    "                if self.error_count < 3:\n",
    "                    self.authenticate()\n",
    "                    self.error_count += 1\n",
    "                    return self.auth_request(url=url)\n",
    "                else:\n",
    "                    raise Exception('Error while authorize passport.moex.com')\n",
    "        except httpx.RequestError as exc:\n",
    "            self.error_count += 1\n",
    "            if self.error_count < 3:\n",
    "                return self.auth_request(url=url)\n",
    "            else:\n",
    "                print(f\"An error occurred while requesting {exc.request.url!r}.\")\n",
    "                return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем экземпляр класса для авторизации запросов к ISS MOEX\n",
    "auth = PassportMOEXAuth(username=os.getenv('LOGIN'), password=os.getenv('PASSWORD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генератор дат\n",
    "def date_range(start_date: date, end_date: date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Маркеты и эндпоинты, по которым проходит валидация\n",
    "class Markets(enum.Enum):\n",
    "    \"\"\"\n",
    "    Типы рынков.\n",
    "    \"\"\"\n",
    "    SHARES = 'eq'\n",
    "    CURRENCY = 'fx'\n",
    "    FUTURES = 'fo'\n",
    "\n",
    "\n",
    "class Endpoints(enum.Enum):\n",
    "    \"\"\"\n",
    "    API эндпоинты.\n",
    "    \"\"\"\n",
    "    TRADESTATS = 'tradestats'\n",
    "    ORDERSTATS = 'orderstats'\n",
    "    ORDERBOOKSTATS = 'obstats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Персеты для билда url для запроса к ISS\n",
    "iss_market_url = {\n",
    "        \"eq\": (\"stock\", \"shares\", \"tqbr\"),\n",
    "        \"fx\": (\"currency\", \"selt\", \"cets\"),\n",
    "        \"fo\": (\"futures\", \"forts\", \"rfud\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем папку для отчетов\n",
    "if os.path.exists('report'):\n",
    "    shutil.rmtree('report')\n",
    "\n",
    "os.makedirs(os.path.join('report', '5min_report'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция определения торгового дня для маркета - обращается к исторической маркетдате и смотрит наличие сделок\n",
    "def is_trading_day_for_market(market: Markets, trading_date: date):\n",
    "    url = f\"https://iss.moex.com/iss/history/engines/{iss_market_url[market][0]}/markets/{iss_market_url[market][1]}/boards/{iss_market_url[market][2]}/securities.json?date={trading_date.strftime('%Y-%m-%d')}\"\n",
    "    data = auth.auth_request(url=url)\n",
    "    df = pd.DataFrame(data['history']['data'], columns=data['history']['columns'])\n",
    "    return False if df[\"NUMTRADES\"].sum() == 0 else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения количества сделок в торговом дне для маркета\n",
    "def get_history_numtrades_for_market(market: Markets, trading_date: date):\n",
    "    info = dict()\n",
    "    url = f\"https://iss.moex.com/iss/history/engines/{iss_market_url[market][0]}/markets/{iss_market_url[market][1]}/boards/{iss_market_url[market][2]}/securities.json?date={trading_date.strftime('%Y-%m-%d')}\"\n",
    "    data = auth.auth_request(url=url)\n",
    "    df = pd.DataFrame(data['history']['data'], columns=data['history']['columns'])\n",
    "    for secid in df['secid'].unique():\n",
    "        info[secid] = df[df['secid'] == secid]['NUMTRADES'].sum()\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"missed_days.json\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(\"tickers_count.json\", 'w') as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_ticker_count_by_day(\n",
    "    df: pd.DataFrame, \n",
    "    report_df: pd.DataFrame, \n",
    "    market: Markets, \n",
    "    trading_date: date\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Разделяет датафрейм по рынкам для подсчета количества тикеров в день для определенного рынка.\n",
    "\n",
    "    :param df: Исходный датафрейм с данными о тикерах.\n",
    "    :param report_df: Датафрейм для отчета, в который будут записываться результаты.\n",
    "    :param market: Объект, представляющий рынок, для которого нужно выполнить расчет.\n",
    "    :param trading_date: Дата, для которой производится расчет.\n",
    "    :return: Обновленный датафрейм отчета с добавленной информацией о тикерах.\n",
    "    \"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if df.empty:\n",
    "        report_df['secid'] = pd.Series(dtype=str)\n",
    "        report_df.set_index('secid', inplace=True)\n",
    "    \n",
    "    info: Dict[str, int] = get_history_numtrades_for_market(market, trading_date)\n",
    "    info_df: pd.DataFrame = pd.DataFrame(list(info.items()), columns=['secid', 'numtrades'])\n",
    "\n",
    "    report_df[trading_date] = False\n",
    "    \n",
    "    valid_info_df: pd.DataFrame = info_df[info_df['numtrades'] != 0]\n",
    "\n",
    "    for _, row in valid_info_df.iterrows():\n",
    "        secid: str = row['secid']\n",
    "        if secid in df['secid'].values:\n",
    "            report_df.at[secid, trading_date] = True\n",
    "        else:\n",
    "            report_df.at[secid, trading_date] = False\n",
    "\n",
    "    for secid in valid_info_df['secid'].values:\n",
    "        if secid not in report_df.index:\n",
    "            report_df.loc[secid] = {trading_date: False}\n",
    "\n",
    "    report_df.index = report_df.index.astype(str)\n",
    "    report_df.columns = pd.to_datetime(report_df.columns)\n",
    "\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_5min_tickers(\n",
    "    df: pd.DataFrame, \n",
    "    market: str, \n",
    "    endpoint: str, \n",
    "    date: Union[datetime, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Подсчитывает количество уникальных тикеров для каждой 5-минутки для заданного датафрейма, \n",
    "    рынка, конечной точки и даты.\n",
    "\n",
    "    :param df: Датафрейм с данными о торговле.\n",
    "    :param market: Название рынка.\n",
    "    :param endpoint: Конечная точка.\n",
    "    :param date: Дата торгов.\n",
    "    :return: Датафрейм с количеством тикеров для каждого 5-минутного интервала.\n",
    "    \"\"\"\n",
    "    df_tickers_count_store: pd.DataFrame = pd.DataFrame()\n",
    "    df['tradedatetime'] = pd.to_datetime(df['tradedate'] + ' ' + df['tradetime'])\n",
    "    df = df.set_index('tradedatetime')\n",
    "\n",
    "    start_time: pd.Timestamp = df.index.min()\n",
    "    end_time: pd.Timestamp = df.index.max()\n",
    "    all_intervals: pd.DatetimeIndex = pd.date_range(start=start_time, end=end_time, freq='5min')\n",
    "\n",
    "    for time_interval in all_intervals:\n",
    "        interval_str: str = time_interval.strftime('%H:%M')\n",
    "        interval_data: pd.DataFrame = df.between_time(time_interval.time(), (time_interval + pd.Timedelta(minutes=5)).time())\n",
    "        ticker_count: int = interval_data['secid'].nunique() if not interval_data.empty else 0\n",
    "        df_tickers_count: pd.DataFrame = pd.DataFrame({\n",
    "            'date': [date], \n",
    "            'market': [market], \n",
    "            'endpoint': [endpoint], \n",
    "            'interval': [interval_str], \n",
    "            'ticker_count': [ticker_count]\n",
    "        })\n",
    "        df_tickers_count_store = pd.concat([df_tickers_count_store, df_tickers_count])\n",
    "\n",
    "    return df_tickers_count_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tickers_count_by_day(\n",
    "    data: pd.DataFrame, \n",
    "    markets: List[str] = ['fx', 'fo', 'eq'], \n",
    "    filepath: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Строит график количества тикеров по дням и типам рынков.\n",
    "\n",
    "    :param data: Датафрейм с данными о тикерах.\n",
    "    :param markets: Список рынков для фильтрации данных.\n",
    "    :param filepath: Путь для сохранения графика. Если None, график будет отображен.\n",
    "    \"\"\"\n",
    "    data_filtered: pd.DataFrame = data[data['market'].isin(markets)]\n",
    "    data_filtered['date'] = pd.to_datetime(data_filtered['date'])\n",
    "    dates: List[pd.Timestamp] = sorted(data_filtered['date'].unique())\n",
    "    market_data: dict = {market: {stats: [] for stats in data_filtered['endpoint'].unique()} for market in markets}\n",
    "    \n",
    "    grouped: pd.DataFrame = data_filtered.groupby(['date', 'market', 'endpoint']).sum().reset_index()\n",
    "    for date in dates:\n",
    "        for market in markets:\n",
    "            for stats in data_filtered['endpoint'].unique():\n",
    "                value: pd.Series = grouped[(grouped['date'] == date) & (grouped['market'] == market) & (grouped['endpoint'] == stats)]['count']\n",
    "                if not value.empty:\n",
    "                    market_data[market][stats].append(value.values[0])\n",
    "                else:\n",
    "                    market_data[market][stats].append(0)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    for market in markets:\n",
    "        for stats in data_filtered['endpoint'].unique():\n",
    "            if any(market_data[market][stats]):\n",
    "                ax.plot(dates, market_data[market][stats], marker='o', label=f'{market.upper()} - {stats}')\n",
    "\n",
    "    ax.set_title('Количество тикеров по дням и типам рынков')\n",
    "    ax.set_xlabel('Дни')\n",
    "    ax.set_ylabel('Количество тикеров')\n",
    "    \n",
    "    ax.set_xticks(dates)\n",
    "    ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in dates], rotation=90)\n",
    "    \n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    ax.grid(True)\n",
    "\n",
    "    table_data: List[List[Union[str, float]]] = []\n",
    "    for market in markets:\n",
    "        for stats in data_filtered['endpoint'].unique():\n",
    "            if any(market_data[market][stats]):\n",
    "                min_value: float = min(market_data[market][stats])\n",
    "                max_value: float = max(market_data[market][stats])\n",
    "                average_value: float = sum(market_data[market][stats]) / len(market_data[market][stats])\n",
    "                median_value: float = statistics.median(market_data[market][stats])\n",
    "\n",
    "                table_data.append([f'{market.upper()} - {stats}', min_value, max_value, average_value, median_value])\n",
    "\n",
    "    table_columns: List[str] = [\"Market - Stats\", \"Min\", \"Max\", \"Average\", \"Median\"]\n",
    "    table = plt.table(cellText=table_data, colLabels=table_columns, cellLoc='center', loc='bottom', bbox=[0, -0.5, 1, 0.3])\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.3, right=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_5min_tickers(\n",
    "    df: pd.DataFrame, \n",
    "    date: date, \n",
    "    market: str, \n",
    "    endpoint: str, \n",
    "    filepath: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Строит график количества тикеров по 5-минутным интервалам для заданной даты, рынка и конечной точки.\n",
    "\n",
    "    :param df: Датафрейм с данными о тикерах.\n",
    "    :param date: Дата для фильтрации данных.\n",
    "    :param market: Название рынка для фильтрации данных.\n",
    "    :param endpoint: Конечная точка для фильтрации данных.\n",
    "    :param filepath: Путь для сохранения графика. Если None, график будет отображен.\n",
    "    \"\"\"\n",
    "    filtered_df: pd.DataFrame = df[(df['date'] == date.strftime('%Y-%m-%d')) & (df['market'] == market) & (df['endpoint'] == endpoint)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(f\"Нет данных для {date}, {market}, {endpoint}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(filtered_df['interval'], filtered_df['ticker_count'], marker='o', linestyle='-', color='b')\n",
    "    \n",
    "    plt.title(f\"Количество тикеров по 5-минуткам для {date}, {market}, {endpoint}\")\n",
    "    plt.xlabel(\"Интервал\")\n",
    "    plt.ylabel(\"Количество тикеров\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "\n",
    "    min_count: float = filtered_df['ticker_count'].min()\n",
    "    max_count: float = filtered_df['ticker_count'].max()\n",
    "    avg_count: float = filtered_df['ticker_count'].mean()\n",
    "    med_count: float = filtered_df['ticker_count'].median()\n",
    "    \n",
    "    stats_data: list[list[float | str]] = [\n",
    "        ['Min', min_count], \n",
    "        ['Max', max_count], \n",
    "        ['Average', avg_count], \n",
    "        ['Median', med_count]\n",
    "    ]\n",
    "    plt.table(\n",
    "        cellText=stats_data, \n",
    "        colLabels=['Statistic', 'Value'], \n",
    "        cellLoc='center', \n",
    "        loc='bottom', \n",
    "        bbox=[0, -0.7, 1, 0.35]\n",
    "    )\n",
    "    \n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_negative_values(series: pd.Series, metric: str) -> Dict[Any, str]:\n",
    "    \"\"\"\n",
    "    Проверяет наличие отрицательных значений или значений None в серии и возвращает словарь с индексами таких значений и метрикой.\n",
    "\n",
    "    :param series: Серия pandas для проверки.\n",
    "    :param metric: Строка, представляющая метрику, которая будет добавлена в результат.\n",
    "    :return: Словарь, где ключи - это индексы значений None или отрицательных значений, а значения - это метрика.\n",
    "    \"\"\"\n",
    "    result: Dict[Any, str] = {}\n",
    "    for index, value in series.items():\n",
    "        if value is None or value < 0:\n",
    "            result[index] = metric\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_range(series: pd.Series, metric: str) -> Dict[Any, str]:\n",
    "    \"\"\"\n",
    "    Проверяет значения в серии на соответствие заданному диапазону [-1, 1]. \n",
    "    Возвращает словарь с индексами значений, выходящих за пределы диапазона, и метрикой.\n",
    "\n",
    "    :param series: Серия pandas, содержащая значения для проверки.\n",
    "    :param metric: Строка, представляющая метрику, которая будет добавлена в результат.\n",
    "    :return: Словарь, где ключи - это индексы значений, выходящих за пределы диапазона [-1, 1], а значения - это метрика.\n",
    "    \"\"\"\n",
    "    out_of_range_values: Dict[Any, str] = {}\n",
    "    for index, value in series.items():\n",
    "        if value is not None and (value < -1 or value > 1):\n",
    "            out_of_range_values[index] = metric\n",
    "\n",
    "    return out_of_range_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_range_negative_data(\n",
    "    df: pd.DataFrame,\n",
    "    market: str,\n",
    "    endpoint: str\n",
    ") -> Union[bool, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Проверяет значения в датафрейме на соответствие диапазону и отрицательным значениям \n",
    "    в зависимости от типа рынка и конечной точки. Возвращает датафрейм с результатами проверки \n",
    "    или False, если нет данных для проверки.\n",
    "\n",
    "    :param df: Датафрейм с данными для проверки.\n",
    "    :param market: Тип рынка (например, 'SHARES', 'CURRENCY', 'FUTURES').\n",
    "    :param endpoint: Конечная точка (например, 'TRADESTATS', 'ORDERSTATS', 'ORDERBOOKSTATS').\n",
    "    :return: Датафрейм с результатами проверки или False, если нет данных для проверки.\n",
    "    \"\"\"\n",
    "    result_list: List[Dict[str, Union[str, Dict[str, str]]]] = []\n",
    "\n",
    "    grouped_dfs: pd.DataFrameGroupBy = df.groupby('secid')\n",
    "\n",
    "    for secid, group_df in grouped_dfs:\n",
    "        columns_to_validate: List[str] = []\n",
    "\n",
    "        if market == Markets.SHARES.value and endpoint == Endpoints.TRADESTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'pr_open', 'pr_high', 'pr_low', 'pr_close', 'pr_std', 'vol', 'val', 'trades', 'pr_vwap',\n",
    "                'trades_b', 'trades_s', 'val_b', 'val_s', 'vol_b', 'vol_s', 'pr_vwap_b', 'pr_vwap_s', 'disb'\n",
    "            ]\n",
    "        elif market == Markets.SHARES.value and endpoint == Endpoints.ORDERSTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'put_orders_b', 'put_orders_s', 'put_val_b', 'put_val_s', 'put_vol_b', 'put_vol_s',\n",
    "                'put_vwap_b', 'put_vwap_s', 'put_vol', 'put_val', 'put_orders', 'cancel_orders_b',\n",
    "                'cancel_orders_s', 'cancel_val_b', 'cancel_val_s', 'cancel_vol_b', 'cancel_vol_s',\n",
    "                'cancel_vwap_b', 'cancel_vwap_s', 'cancel_vol', 'cancel_val', 'cancel_orders'\n",
    "            ]\n",
    "        elif market == Markets.SHARES.value and endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'spread_bbo', 'spread_lv10', 'spread_1mio', 'levels_b', 'levels_s', 'vol_b',\n",
    "                'vol_s', 'val_b', 'val_s', 'imbalance_vol_bbo', 'imbalance_val_bbo', 'imbalance_vol',\n",
    "                'imbalance_val', 'vwap_b', 'vwap_s', 'vwap_b_1mio', 'vwap_s_1mio'\n",
    "            ]\n",
    "        elif market == Markets.CURRENCY.value and endpoint == Endpoints.TRADESTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'pr_open', 'pr_close', 'pr_high', 'pr_low', 'pr_std', 'vol', 'val', 'trades',\n",
    "                'pr_vwap', 'trades_b', 'trades_s', 'val_b', 'val_s', 'vol_b', 'vol_s', 'disb'\n",
    "            ]\n",
    "        elif market == Markets.CURRENCY.value and endpoint == Endpoints.ORDERSTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'put_orders_b', 'put_orders_s', 'put_val_b', 'put_val_s', 'put_vol_b', 'put_vol_s',\n",
    "                'put_vwap_b', 'put_vwap_s', 'cancel_orders_b', 'cancel_orders_s', 'cancel_val_b',\n",
    "                'cancel_val_s', 'cancel_vol_b', 'cancel_vol_s', 'cancel_vwap_b', 'cancel_vwap_s'\n",
    "            ]\n",
    "        elif market == Markets.CURRENCY.value and endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'spread_l1', 'spread_l2', 'spread_l3', 'spread_l5', 'spread_l10', 'levels_b',\n",
    "                'levels_s', 'vol_b_l1', 'vol_b_l2', 'vol_b_l3', 'vol_b_l5', 'vol_b_l10', 'vol_s_l1',\n",
    "                'vol_s_l2', 'vol_s_l3', 'vol_s_l5', 'vol_s_l10', 'vol_s_l20', 'vwap_b_l3', 'vwap_b_l5',\n",
    "                'vwap_b_l10', 'vwap_s_l3', 'vwap_s_l5', 'vwap_s_l10'\n",
    "            ]\n",
    "        elif market == Markets.FUTURES.value and endpoint == Endpoints.TRADESTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'pr_open', 'pr_high', 'pr_low', 'pr_close', 'pr_std', 'vol', 'val', 'trades', 'pr_vwap',\n",
    "                'trades_b', 'trades_s', 'val_b', 'val_s', 'vol_b', 'vol_s', 'disb', 'pr_vwap_b', 'pr_vwap_s'\n",
    "            ]\n",
    "        elif market == Markets.FUTURES.value and endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "            columns_to_validate = [\n",
    "                'mid_price', 'micro_price', 'spread_l1', 'spread_l2', 'spread_l3', 'spread_l5', 'spread_l10',\n",
    "                'spread_l20', 'levels_b', 'levels_s', 'vol_b_l1', 'vol_b_l2', 'vol_b_l3', 'vol_b_l5', 'vol_b_l10',\n",
    "                'vol_b_l20', 'vol_s_l1', 'vol_s_l2', 'vol_s_l3', 'vol_s_l5', 'vol_s_l10', 'vol_s_l20', 'vwap_b_l3',\n",
    "                'vwap_b_l5', 'vwap_b_l10', 'vwap_b_l20', 'vwap_s_l3', 'vwap_s_l5', 'vwap_s_l10', 'vwap_s_l20'\n",
    "            ]\n",
    "\n",
    "        invalid_columns: List[Dict[str, str]] = []\n",
    "                    \n",
    "        for metric in columns_to_validate:\n",
    "            if 'disb' == metric or 'imbalance' in metric:\n",
    "                issue = check_range(group_df[metric], metric)\n",
    "            else:\n",
    "                issue = check_negative_values(group_df[metric], metric)\n",
    "\n",
    "            if issue:\n",
    "                issue = {group_df.at[index, 'tradetime']: value for index, value in issue.items()}\n",
    "                if issue:\n",
    "                    invalid_columns.append(issue)\n",
    "\n",
    "        if invalid_columns:\n",
    "            for invalid_column in invalid_columns:\n",
    "                result_list.append({\n",
    "                    'date': df['date'].iloc[0],\n",
    "                    'market': market,\n",
    "                    'endpoint': endpoint,\n",
    "                    'secid': secid,\n",
    "                    'invalid_column': invalid_column\n",
    "                })\n",
    "\n",
    "    negative_range_report: pd.DataFrame = pd.DataFrame(result_list)\n",
    "    \n",
    "    if 'invalid_column' not in negative_range_report.columns:\n",
    "        return False\n",
    "    \n",
    "    negative_range_report = negative_range_report[negative_range_report['invalid_column'] != True]\n",
    "    subset_columns: List[str] = [\"date\", \"market\", \"endpoint\", \"secid\"]\n",
    "    negative_range_report = negative_range_report.drop_duplicates(subset=subset_columns)\n",
    "    \n",
    "    return negative_range_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decimals():\n",
    "    \"\"\"\n",
    "    Получает количество знаков после запятой для каждого тикера на различных рынках.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    tuple\n",
    "        Кортеж, содержащий три словаря: для акций, валют и фьючерсов.\n",
    "    \"\"\"\n",
    "    eq_decimals = dict()\n",
    "    fx_decimals = dict()\n",
    "    fo_decimals = dict()\n",
    "\n",
    "    def update_decimals(url, decimals_dict):\n",
    "        data = auth.auth_request(url=url)\n",
    "        \n",
    "        columns = data['securities']['columns']\n",
    "        data_rows = data['securities']['data']\n",
    "        \n",
    "        secid_index = columns.index('SECID')\n",
    "        decimals_index = columns.index('DECIMALS')\n",
    "        \n",
    "        for row in data_rows:\n",
    "            secid = row[secid_index]\n",
    "            decimals = row[decimals_index]\n",
    "            decimals_dict[secid] = decimals\n",
    "\n",
    "    eq_url = 'https://iss.moex.com/iss/engines/stock/markets/shares/boards/tqbr/securities.json'\n",
    "    fx_url = 'https://iss.moex.com/iss/engines/currency/markets/selt/boards/cets/securities.json'\n",
    "    fo_url = 'https://iss.moex.com/iss/engines/futures/markets/forts/boards/rfud/securities.json'\n",
    "\n",
    "    update_decimals(eq_url, eq_decimals)\n",
    "    update_decimals(fx_url, fx_decimals)\n",
    "    update_decimals(fo_url, fo_decimals)\n",
    "\n",
    "    return eq_decimals, fx_decimals, fo_decimals\n",
    "\n",
    "eq_decimals, fx_decimals, fo_decimals = get_decimals()\n",
    "tickers_decimal = eq_decimals | fx_decimals | fo_decimals\n",
    "tickers_decimal['SBER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_rounding(series: pd.Series, decimal: int) -> bool:\n",
    "    \"\"\"\n",
    "    Проверяет, правильно ли округлены значения в серии данных.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Серия данных для проверки округления.\n",
    "    decimal : int\n",
    "        Количество знаков после запятой для округления.\n",
    "\n",
    "    Возвращает:\n",
    "    ----------\n",
    "    bool\n",
    "        True, если все значения правильно округлены, в противном случае False.\n",
    "    \"\"\"\n",
    "    for value in series:\n",
    "        rounded_value = round(value, decimal)\n",
    "        if rounded_value != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "a = pd.Series([1.22])\n",
    "\n",
    "validate_rounding(series=a, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_decimals(dataset: Dict[str, Dict[str, Dict[str, pd.DataFrame]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Проверяет, что значения в датафреймах округлены до корректного числа десятичных знаков в зависимости от типа рынка и конечной точки.\n",
    "\n",
    "    :param dataset: Словарь, содержащий данные для проверки. Ключи - даты, рынки и конечные точки, значения - датафреймы с данными.\n",
    "    :return: Датафрейм с результатами проверки. Каждый результат включает дату, рынок, конечную точку, идентификатор секьюрити и некорректные столбцы.\n",
    "    \"\"\"\n",
    "    result_list: List[Dict[str, Union[str, int]]] = []\n",
    "\n",
    "    for date, markets in dataset.items():\n",
    "        for market, endpoints in markets.items():\n",
    "            for endpoint, df in endpoints.items():\n",
    "                print(f\"{date} | {market} | {endpoint}\")\n",
    "\n",
    "                grouped_dfs = df.groupby('secid')\n",
    "\n",
    "                for secid, group_df in grouped_dfs:\n",
    "                    decimal_places = tickers_decimal.get(secid, 2)\n",
    "                    group_df.fillna(0, inplace=True)\n",
    "\n",
    "                    columns_to_validate = {}\n",
    "\n",
    "                    if market == Markets.SHARES.value:\n",
    "                        if endpoint == Endpoints.TRADESTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"pr_open\": decimal_places, \"pr_high\": decimal_places, \"pr_low\": decimal_places,\n",
    "                                \"pr_close\": decimal_places, \"pr_change\": 4, \"pr_vwap\": decimal_places,\n",
    "                                \"pr_vwap_b\": decimal_places, \"pr_vwap_s\": decimal_places, \"pr_std\": 4, \"disb\": 2\n",
    "                            }\n",
    "                        elif endpoint == Endpoints.ORDERSTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"put_vwap_b\": decimal_places, \"put_vwap_s\": decimal_places,\n",
    "                                \"cancel_vwap_b\": decimal_places, \"cancel_vwap_s\": decimal_places,\n",
    "                            }\n",
    "                        elif endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"mid_price\": decimal_places, \"micro_price\": decimal_places,\n",
    "                                \"spread_l1\": decimal_places, \"spread_l2\": decimal_places,\n",
    "                                \"spread_l3\": decimal_places, \"spread_l5\": decimal_places,\n",
    "                                \"spread_l10\": decimal_places, \"vwap_b_l3\": decimal_places,\n",
    "                                \"vwap_b_l5\": decimal_places, \"vwap_b_l10\": decimal_places,\n",
    "                                \"vwap_s_l3\": decimal_places, \"vwap_s_l5\": decimal_places,\n",
    "                                \"vwap_s_l10\": decimal_places,\n",
    "                            }\n",
    "\n",
    "                    elif market == Markets.CURRENCY.value:\n",
    "                        if endpoint == Endpoints.TRADESTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"pr_open\": decimal_places, \"pr_high\": decimal_places, \"pr_low\": decimal_places,\n",
    "                                \"pr_close\": decimal_places, \"pr_change\": 4, \"pr_vwap\": decimal_places,\n",
    "                                \"pr_vwap_b\": decimal_places, \"pr_vwap_s\": decimal_places, \"pr_std\": 4, \"disb\": 2\n",
    "                            }\n",
    "                        elif endpoint == Endpoints.ORDERSTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"put_vwap_b\": decimal_places, \"put_vwap_s\": decimal_places,\n",
    "                                \"cancel_vwap_b\": decimal_places, \"cancel_vwap_s\": decimal_places,\n",
    "                            }\n",
    "                        elif endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"vwap_b\": decimal_places, \"vwap_s\": decimal_places,\n",
    "                                \"vwap_b_1mio\": decimal_places, \"vwap_s_1mio\": decimal_places,\n",
    "                            }\n",
    "\n",
    "                    elif market == Markets.FUTURES.value:\n",
    "                        if endpoint == Endpoints.TRADESTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"pr_open\": decimal_places, \"pr_high\": decimal_places, \"pr_low\": decimal_places,\n",
    "                                \"pr_close\": decimal_places, \"pr_change\": 4, \"pr_vwap\": decimal_places,\n",
    "                                \"pr_vwap_b\": decimal_places, \"pr_vwap_s\": decimal_places, \"pr_std\": 4, \"disb\": 2\n",
    "                            }\n",
    "                        elif endpoint == Endpoints.ORDERBOOKSTATS.value:\n",
    "                            columns_to_validate = {\n",
    "                                \"mid_price\": decimal_places, \"micro_price\": decimal_places,\n",
    "                                \"spread_l1\": 1, \"spread_l2\": 1, \"spread_l3\": 1, \"spread_l5\": 1, \"spread_l10\": 1,\n",
    "                                \"spread_l20\": 1, \"vwap_b_l3\": decimal_places, \"vwap_b_l5\": decimal_places,\n",
    "                                \"vwap_b_l10\": decimal_places, \"vwap_b_l20\": decimal_places,\n",
    "                                \"vwap_s_l3\": decimal_places, \"vwap_s_l5\": decimal_places,\n",
    "                                \"vwap_s_l10\": decimal_places, \"vwap_s_l20\": decimal_places,\n",
    "                            }\n",
    "\n",
    "                    invalid_columns = []\n",
    "\n",
    "                    for column, decimal in columns_to_validate.items():\n",
    "                        if column in group_df.columns and not validate_rounding(group_df[column], decimal):\n",
    "                            invalid_columns.append(column)\n",
    "\n",
    "                    if invalid_columns:\n",
    "                        for column in invalid_columns:\n",
    "                            result_list.append({\n",
    "                                'date': date,\n",
    "                                'market': market,\n",
    "                                'endpoint': endpoint,\n",
    "                                'secid': secid,\n",
    "                                'invalid_column': column\n",
    "                            })\n",
    "\n",
    "    return pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(start: date, end: date):\n",
    "    eq_slice_reports_df = None\n",
    "    fx_slice_reports_df = None\n",
    "    fo_slice_reports_df = None\n",
    "\n",
    "    sliced_reports = {\n",
    "        \"eq\": [eq_slice_reports_df, f\"sliced_eq_reports_{start}_{end}.csv\"],\n",
    "        \"fx\": [fx_slice_reports_df, f\"sliced_fx_reports_{start}_{end}.csv\"],\n",
    "        \"fo\": [fo_slice_reports_df, f\"sliced_fo_reports_{start}_{end}.csv\"],\n",
    "    }\n",
    "\n",
    "    # Проходим по каждому дню маркету и эндпоинту\n",
    "    for trading_date in date_range(start_date=start, end_date=end):\n",
    "        formatted_date = trading_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        for market in Markets:\n",
    "            for endpoint in Endpoints:\n",
    "                # Пропускаем FO orderstats\n",
    "                if market == Markets.FUTURES and endpoint == Endpoints.ORDERSTATS:\n",
    "                    continue\n",
    "                \n",
    "                trading_day_status = is_trading_day_for_market(market=market.value, trading_date=trading_date)\n",
    "\n",
    "                # Если не торговый день или не было сделок - то скипаем\n",
    "                if not trading_day_status:\n",
    "                    continue\n",
    "\n",
    "                url = f'https://iss.moex.com/iss/datashop/algopack/{market.value}/{endpoint.value}.json?date={formatted_date}'\n",
    "                \n",
    "                all_data = []\n",
    "                page_num = 0\n",
    "                \n",
    "                # Получаем данные за день\n",
    "                while True:\n",
    "                    page_url = f'{url}&start={page_num * 1000}'\n",
    "                    data = auth.auth_request(url=page_url)\n",
    "                    columns = data['data']['columns']\n",
    "                    page_data = data['data']['data']\n",
    "                    \n",
    "                    if not page_data or page_data is None:\n",
    "                        break\n",
    "                    \n",
    "                    all_data.extend(page_data)\n",
    "                    page_num += 1\n",
    "                    print(f\"{market.value} | {endpoint.value} | {formatted_date}: page num: \", page_num, end='\\n')\n",
    "\n",
    "                df = pd.DataFrame(all_data, columns=columns)\n",
    "                \n",
    "                # Если данных нет, то записываем в пропущенный день\n",
    "                if df.empty:\n",
    "                    missed_days = read_json_file(\"missed_days.json\")\n",
    "                    missed_days.append({'date': formatted_date, 'market': market.value, 'endpoint': endpoint.value})\n",
    "                    with open(\"missed_days.json\", 'w') as f:\n",
    "                        json.dump(missed_days, f, indent=4)\n",
    "                    continue\n",
    "                \n",
    "                # считаем кол-во уникальных тикеров и дампим в json\n",
    "                unique_tickers_count = df['secid'].nunique()\n",
    "                tickers_count = read_json_file(\"tickers_count.json\")\n",
    "                tickers_count.append({'date': formatted_date, 'market': market.value, 'endpoint': endpoint.value, 'count': unique_tickers_count})\n",
    "                with open(\"tickers_count.json\", 'w') as f:\n",
    "                    json.dump(tickers_count, f, indent=4)\n",
    "\n",
    "                # считаем кол-во тикеров для 5минуток + график   \n",
    "                count_5min_df = count_5min_tickers(df=df, market=market.value, endpoint=endpoint.value, date=formatted_date)\n",
    "                count_5min_df.to_csv(f\"report/5min_report/{formatted_date}_{market.value}_{endpoint.value}.csv\", index=False, sep=\";\")\n",
    "                plot_5min_tickers(df=count_5min_df, date=trading_date, market=market.value, endpoint=endpoint.value, filepath=f\"report/5min_report/{formatted_date}_{market.value}_{endpoint.value}.png\")\n",
    "\n",
    "                # Валидируем рендж для imbalance и тд + неотрицательность для ценовых параметров и прочих не отрицательных значений\n",
    "                negative_range_report = validate_range_negative_data(df=df, market=market.value, endpoint=endpoint.value)\n",
    "                if negative_range_report is not False:\n",
    "                    negative_range_report.to_csv(f\"report/negative_range_report/{formatted_date}_{market.value}_{endpoint.value}.csv\", index=False, sep=\";\")\n",
    "\n",
    "                # Валидация округлений цен инструментов \n",
    "                # Нужно делать из внутренней сети, тк Decimals не отображается вне контура MOEX \n",
    "                \n",
    "                # dataset = {\n",
    "                #     formatted_date: {\n",
    "                #         market.value: {\n",
    "                #             endpoint.value: df\n",
    "                #         }\n",
    "                #     }\n",
    "                # }\n",
    "                # decimals_report = validate_decimals(dataset=dataset)\n",
    "                # if not decimals_report.empty:\n",
    "                #     decimals_report.to_csv(f\"report/decimals_report/{formatted_date}_{market.value}_{endpoint.value}.csv\", index=False, sep=\";\")b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск отчета\n",
    "START_DATE=date(2024, 6, 1)\n",
    "END_DATE=date(2024, 6, 3)\n",
    "get_dataset(start=START_DATE, end=END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формуируем отчет по пропущенным дням\n",
    "missed_days = read_json_file(\"missed_days.json\")\n",
    "if len(missed_days) > 0:\n",
    "    missed_days_df = pd.DataFrame(missed_days)\n",
    "    missed_days_df = missed_days_df.groupby(['date', 'market'])['endpoint'].apply(list).reset_index()\n",
    "    missed_days_df.to_csv(\"report/missed_days.csv\", index=False, sep=\";\")\n",
    "    missed_days_df\n",
    "else:\n",
    "    with open(\"report/missed_days.csv\", 'w') as f:\n",
    "        f.write(\"No missed days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формуируем отчет по количеству тикеров\n",
    "with open(\"tickers_count.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "tickers_count_df = pd.DataFrame(data)\n",
    "tickers_count_df.to_csv(\"report/tickers_count.csv\", index=False, sep=\";\")\n",
    "tickers_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tickers_count_by_day(data=tickers_count_df, markets=['eq', 'fx', 'fo'], filepath=\"report/tickers_count_by_day.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tickers_count_by_day(data=tickers_count_df, markets=['fx'], filepath=\"report/tickers_count_by_day_fx.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tickers_count_by_day(data=tickers_count_df, markets=['fo'], filepath=\"report/tickers_count_by_day_fo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tickers_count_by_day(data=tickers_count_df, markets=['eq'], filepath=\"report/tickers_count_by_day_eq.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
